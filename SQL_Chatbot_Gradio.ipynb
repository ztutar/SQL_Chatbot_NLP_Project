{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "onPbOp5CeQc6"
      },
      "outputs": [],
      "source": [
        "!pip install gradio langchain_core langchain_community ollama\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YSFpGOeII8RH"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import ollama as Ollama\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.pool import StaticPool\n",
        "import sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uWDTIp98bnR_"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "clear_output()\n",
        "\n",
        "#Create a Python script to start the Ollama API server in a seperate thread\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "  os.environ['OLLAMA_HOST'] = '127.0.0.1:11434'\n",
        "  os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "  subprocess.Popen(['ollama', 'serve'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VrLB3Fj2wXuf"
      },
      "outputs": [],
      "source": [
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cqot1txmtzVH"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.1:70b\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "__l3dh04_VXp"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.1:8b\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tIQO8-IkIzAM"
      },
      "outputs": [],
      "source": [
        "# Global variable to store the database connection object\n",
        "db = None\n",
        "\n",
        "# Function to connect to the database\n",
        "def connectDatabase(url):\n",
        "    response = requests.get(url)\n",
        "    sql_script = response.text\n",
        "\n",
        "    connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n",
        "    connection.executescript(sql_script)\n",
        "    engine = create_engine(\n",
        "        \"sqlite://\",\n",
        "        creator=lambda: connection,\n",
        "        poolclass=StaticPool,\n",
        "        connect_args={\"check_same_thread\": False})\n",
        "    db = SQLDatabase(engine)\n",
        "\n",
        "\n",
        "# Function to run a query on the database\n",
        "def runQuery(query):\n",
        "  if db:\n",
        "    return db.run(query)\n",
        "  else:\n",
        "    return \"Please connect to the database first.\"\n",
        "\n",
        "\n",
        "# Function to get the database schema\n",
        "def getDatabaseSchema():\n",
        "  if db:\n",
        "    return db.get_table_info()\n",
        "  else:\n",
        "    return \"Please connect to the database first.\"\n",
        "\n",
        "\n",
        "def getQueryFromLLM(llm, question, max_iteration=10):\n",
        "    template = \"\"\"below is the schema of SQLite database, read the schema carefully about the table and column names. Also take care of table or column name case sensitivity.\n",
        "    Finally answer user's question in the form of SQL query.\n",
        "\n",
        "    {schema}\n",
        "\n",
        "    please only provide the SQL query and nothing else\n",
        "\n",
        "    for example:\n",
        "    question: how many albums we have in database\n",
        "    SQL query: SELECT COUNT(*) FROM album\n",
        "    question: how many customers are from Brazil in the database ?\n",
        "    SQL query: SELECT COUNT(*) FROM customer WHERE country=Brazil\n",
        "\n",
        "    your turn :\n",
        "    question: {question}\n",
        "    SQL query :\n",
        "    please only provide the SQL query and nothing else\n",
        "    \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)  # Define prompt outside the loop\n",
        "    chain = prompt | llm  # Define initial chain outside the loop\n",
        "    i = max_iteration\n",
        "    response = None  # Initialize response to None\n",
        "\n",
        "    while i>0:\n",
        "        try:\n",
        "            response = chain.invoke({\n",
        "                \"question\": question,\n",
        "                \"schema\": getDatabaseSchema(),\n",
        "                \"error\" : \"\"\n",
        "            })\n",
        "            # Attempt to execute the query to check its validity\n",
        "            result = runQuery(response.content)\n",
        "            # If execution is successful, break the loop\n",
        "            break\n",
        "        except Exception as error:\n",
        "            # If an error occurs, feed the error message back to the LLM\n",
        "            print(f\"Error encountered: {error}\")\n",
        "            template = \"\"\"Previous query attempt failed with error: {error}.\n",
        "                           Please try generating a different SQL query for the question: {question}.\n",
        "                           Here is the database schema for reference: {schema}\n",
        "                           SQL query: \"\"\"\n",
        "            prompt = ChatPromptTemplate.from_template(template)\n",
        "            chain = prompt | llm\n",
        "            response = chain.invoke({  # Invoke the chain with the error message\n",
        "                \"question\": question,\n",
        "                \"schema\": getDatabaseSchema(),\n",
        "                \"error\": str(error)  # Pass the error message to the prompt\n",
        "            })\n",
        "            i -= 1\n",
        "\n",
        "    if response:  # Check if response has been assigned a value\n",
        "      # Check if the response indicates failure\n",
        "      if \"Failed to generate a valid query.\" in response.content:\n",
        "        return None  # Return None to signal query generation failure\n",
        "      else:\n",
        "        return response.content\n",
        "    else:\n",
        "        return None  # Return None if no response was generated\n",
        "\n",
        "\n",
        "def getResponseForQueryResult(llm, question, query, result):\n",
        "    template2 = \"\"\"below is the schema of SQLite database, read the schema carefully about the table and column names of each table.\n",
        "    Also look into the conversation if available\n",
        "    Finally write a response in natural language by looking into the conversation and result.\n",
        "\n",
        "    {schema}\n",
        "\n",
        "    Here are some example for you:\n",
        "    question: how many albums we have in database\n",
        "    SQL query: SELECT COUNT(*) FROM album;\n",
        "    Result : [(34,)]\n",
        "    Response: There are 34 albums in the database.\n",
        "\n",
        "    question: how many users we have in database\n",
        "    SQL query: SELECT COUNT(*) FROM customer;\n",
        "    Result : [(59,)]\n",
        "    Response: There are 59 users in the database.\n",
        "\n",
        "    question: how many users above are from india we have in database\n",
        "    SQL query: SELECT COUNT(*) FROM customer WHERE country=india;\n",
        "    Result : [(4,)]\n",
        "    Response: There are 4 users in the database.\n",
        "\n",
        "    your turn to write response in natural language from the given result :\n",
        "    question: {question}\n",
        "    SQL query : {query}\n",
        "    Result : {result}\n",
        "    Response:\n",
        "    \"\"\"\n",
        "\n",
        "    prompt2 = ChatPromptTemplate.from_template(template2)\n",
        "    chain2 = prompt2 | llm\n",
        "\n",
        "    response = chain2.invoke({\n",
        "        \"question\": question,\n",
        "        \"schema\": getDatabaseSchema(),\n",
        "        \"query\": query,\n",
        "        \"result\": result\n",
        "    })\n",
        "\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LpoUAMclRjBU"
      },
      "outputs": [],
      "source": [
        "def clear_history():\n",
        "    return [], \"\", \"\", \"\"\n",
        "\n",
        "def chat_with_sql(question, database, model, chat_history):\n",
        "    if not database:\n",
        "        return chat_history, \"\", \"Please connect to a database first.\"\n",
        "\n",
        "    chat_history.append((\"user\", question))\n",
        "\n",
        "    try:\n",
        "        print(\"Connecting to the database...\")\n",
        "        connectDatabase(url=database)\n",
        "        print(\"Database connected.\")\n",
        "\n",
        "        print(f\"Loading model: {model}...\")\n",
        "        llm = ChatOllama(model=model, temperature=0.1)\n",
        "        print(\"Model loaded.\")\n",
        "\n",
        "        print(f\"Generating query from LLM for question: {question}\")\n",
        "        query = getQueryFromLLM(llm, question)\n",
        "        print(f\"Generated query: {query}\")\n",
        "\n",
        "        print(\"Running query on the database...\")\n",
        "        result = runQuery(query)\n",
        "        print(f\"Query result: {result}\")\n",
        "\n",
        "        print(\"Generating response based on query result...\")\n",
        "        response = getResponseForQueryResult(llm, question, query, result)\n",
        "        print(f\"Response: {response}\")\n",
        "\n",
        "        chat_history.append((\"assistant\", response))\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred: {str(e)}\"\n",
        "        print(error_message)\n",
        "        chat_history.append((\"assistant\", error_message))\n",
        "\n",
        "    return chat_history, \"\", \"\"\n",
        "\n",
        "models = [model[\"name\"] for model in Ollama.list()[\"models\"]]\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Chat with SQL DB ðŸ¤–\")\n",
        "\n",
        "    with gr.Row():\n",
        "        database = gr.Textbox(label=\"Database\", placeholder=\"ex: https://raw.githubusercontent.com/....sql\")\n",
        "        model = gr.Dropdown(choices=models, label=\"Model\", value=models[0])\n",
        "\n",
        "    chat_history = []\n",
        "\n",
        "    question = gr.Textbox(label=\"Chat with an SQL database\", placeholder=\"Enter your question here...\")\n",
        "\n",
        "    with gr.Row():\n",
        "        connect_btn = gr.Button(\"Connect\")\n",
        "        clear_btn = gr.Button(\"Clear message history\")\n",
        "\n",
        "    chat_output = gr.Chatbot(height=400)\n",
        "\n",
        "    def submit_callback(question, database, model):\n",
        "        return chat_with_sql(question, database, model, chat_history)\n",
        "\n",
        "    question.submit(submit_callback, inputs=[question, database, model], outputs=[chat_output, question, database])\n",
        "    connect_btn.click(submit_callback, inputs=[question, database, model], outputs=[chat_output, question, database])\n",
        "    clear_btn.click(clear_history, inputs=None, outputs=[chat_output, question, database, model])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "hcgAebSJU7MP",
        "outputId": "5475a6d5-6c7f-4d40-9528-46aefadaa811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://8a188e2cc0f5574dd8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://8a188e2cc0f5574dd8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4oE2st4U-JA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
