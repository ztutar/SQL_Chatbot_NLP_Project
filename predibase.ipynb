{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers\n",
    "%pip install -U datasets\n",
    "%pip install -U accelerate\n",
    "%pip install -U peft\n",
    "%pip install -U trl\n",
    "%pip install -U bitsandbytes\n",
    "%pip install -U wandb\n",
    "%pip install -U torch\n",
    "%pip install -U predibase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting predibase\n",
      "  Downloading predibase-2024.8.5.tar.gz (87 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting dataclasses-json==0.5.7 (from predibase)\n",
      "  Downloading dataclasses_json-0.5.7-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting deprecation (from predibase)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting ipyplot (from predibase)\n",
      "  Downloading ipyplot-1.1.2-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: ipython!=8.13.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (8.26.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (2.2.2)\n",
      "Collecting predibase-api==2024.8.5 (from predibase)\n",
      "  Downloading predibase-api-2024.8.5.tar.gz (6.3 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting progress-table==0.1.26 (from predibase)\n",
      "  Downloading progress-table-0.1.26.tar.gz (12 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: protobuf in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (5.27.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (17.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (2.8.2)\n",
      "Collecting pyjwt (from predibase)\n",
      "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (2.32.3)\n",
      "Requirement already satisfied: rich in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (13.8.0)\n",
      "Requirement already satisfied: semantic-version in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (2.10.0)\n",
      "Collecting tabulate (from predibase)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (4.66.5)\n",
      "Collecting tritonclient (from predibase)\n",
      "  Downloading tritonclient-2.49.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typer[all] in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (0.12.5)\n",
      "Collecting urllib3==1.26.12 (from predibase)\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl.metadata (47 kB)\n",
      "Requirement already satisfied: websockets>=11.0.3 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from predibase) (12.0)\n",
      "Collecting lorax-client>=0.6.1 (from predibase)\n",
      "  Downloading lorax_client-0.6.2-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting eval-type-backport (from predibase)\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting openai (from predibase)\n",
      "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting retry (from predibase)\n",
      "  Downloading retry-0.9.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from dataclasses-json==0.5.7->predibase) (3.22.0)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json==0.5.7->predibase)\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from dataclasses-json==0.5.7->predibase) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from progress-table==0.1.26->predibase) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from ipython!=8.13.0->predibase) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from ipython!=8.13.0->predibase) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from ipython!=8.13.0->predibase) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from ipython!=8.13.0->predibase) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from ipython!=8.13.0->predibase) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from ipython!=8.13.0->predibase) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from ipython!=8.13.0->predibase) (5.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from ipython!=8.13.0->predibase) (4.12.2)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.9 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from lorax-client>=0.6.1->predibase) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.12 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from lorax-client>=0.6.1->predibase) (0.24.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from pydantic>=2.0->predibase) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from pydantic>=2.0->predibase) (2.20.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from deprecation->predibase) (24.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from ipyplot->predibase) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from ipyplot->predibase) (10.4.0)\n",
      "Collecting shortuuid (from ipyplot->predibase)\n",
      "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from openai->predibase) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai->predibase)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from openai->predibase) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai->predibase)\n",
      "  Downloading jiter-0.5.0-cp311-none-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from openai->predibase) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from pandas->predibase) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from pandas->predibase) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from python-dateutil->predibase) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from requests->predibase) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from requests->predibase) (3.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from requests->predibase) (2024.7.4)\n",
      "Collecting py<2.0.0,>=1.4.26 (from retry->predibase)\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from rich->predibase) (3.0.0)\n",
      "Collecting python-rapidjson>=0.9.1 (from tritonclient->predibase)\n",
      "  Downloading python_rapidjson-1.20-cp311-cp311-win_amd64.whl.metadata (23 kB)\n",
      "INFO: pip is looking at multiple versions of tritonclient to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tritonclient (from predibase)\n",
      "  Downloading tritonclient-2.48.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading tritonclient-2.47.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading tritonclient-2.46.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading tritonclient-2.45.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading tritonclient-2.44.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading tritonclient-2.43.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading tritonclient-2.42.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "INFO: pip is still looking at multiple versions of tritonclient to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tritonclient-2.41.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from typer[all]->predibase) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from typer[all]->predibase) (1.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (1.9.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai->predibase) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->predibase) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.12->lorax-client>=0.6.1->predibase) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.12->lorax-client>=0.6.1->predibase) (2024.6.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from jedi>=0.16->ipython!=8.13.0->predibase) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->predibase) (0.1.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython!=8.13.0->predibase) (0.2.13)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json==0.5.7->predibase) (1.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from stack-data->ipython!=8.13.0->predibase) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from stack-data->ipython!=8.13.0->predibase) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\matti\\documents\\github\\sql_chatbot_nlp_project\\.venv\\lib\\site-packages (from stack-data->ipython!=8.13.0->predibase) (0.2.3)\n",
      "Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "Downloading lorax_client-0.6.2-py3-none-any.whl (11 kB)\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading ipyplot-1.1.2-py3-none-any.whl (13 kB)\n",
      "Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
      "Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tritonclient-2.41.1-py3-none-any.whl (101 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.5.0-cp311-none-win_amd64.whl (191 kB)\n",
      "Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Downloading python_rapidjson-1.20-cp311-cp311-win_amd64.whl (149 kB)\n",
      "Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: predibase, predibase-api, progress-table\n",
      "  Building wheel for predibase (pyproject.toml): started\n",
      "  Building wheel for predibase (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for predibase: filename=predibase-2024.8.5-py3-none-any.whl size=113901 sha256=003de888c1cd599ae9135ae10957fa7751d9437ba5ae0c8c2e7530a1540d092d\n",
      "  Stored in directory: c:\\users\\matti\\appdata\\local\\pip\\cache\\wheels\\07\\67\\a6\\629b20ca4171c33affbee1ca5d86a40388c485054597d1b6d3\n",
      "  Building wheel for predibase-api (pyproject.toml): started\n",
      "  Building wheel for predibase-api (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for predibase-api: filename=predibase_api-2024.8.5-py3-none-any.whl size=10212 sha256=65739ef570826583b9b62f1045573f2c9773607dc1079039c9e8f893f9256d17\n",
      "  Stored in directory: c:\\users\\matti\\appdata\\local\\pip\\cache\\wheels\\69\\bf\\d2\\3eb40470b4717b2a04e169c341e4fb9915d76640310ea2718d\n",
      "  Building wheel for progress-table (pyproject.toml): started\n",
      "  Building wheel for progress-table (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for progress-table: filename=progress_table-0.1.26-py3-none-any.whl size=12250 sha256=20c3a694a8dc9bb990b6c5fda9675f2f853f0f73e0f294bdd3f17f5e4266e3be\n",
      "  Stored in directory: c:\\users\\matti\\appdata\\local\\pip\\cache\\wheels\\29\\74\\8d\\a81415685be0a00425619b2ca39a794b42b1d2a86cd20e7c1f\n",
      "Successfully built predibase predibase-api progress-table\n",
      "Installing collected packages: urllib3, tabulate, shortuuid, python-rapidjson, pyjwt, py, progress-table, predibase-api, jiter, eval-type-backport, distro, deprecation, tritonclient, retry, marshmallow-enum, openai, dataclasses-json, lorax-client, ipyplot, predibase\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.2\n",
      "    Uninstalling urllib3-2.2.2:\n",
      "      Successfully uninstalled urllib3-2.2.2\n",
      "  Attempting uninstall: dataclasses-json\n",
      "    Found existing installation: dataclasses-json 0.6.7\n",
      "    Uninstalling dataclasses-json-0.6.7:\n",
      "      Successfully uninstalled dataclasses-json-0.6.7\n",
      "Successfully installed dataclasses-json-0.5.7 deprecation-2.1.0 distro-1.9.0 eval-type-backport-0.2.0 ipyplot-1.1.2 jiter-0.5.0 lorax-client-0.6.2 marshmallow-enum-1.5.1 openai-1.42.0 predibase-2024.8.5 predibase-api-2024.8.5 progress-table-0.1.26 py-1.11.0 pyjwt-2.9.0 python-rapidjson-1.20 retry-0.9.2 shortuuid-1.0.13 tabulate-0.9.0 tritonclient-2.41.1 urllib3-1.26.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: typer 0.12.5 does not provide the extra 'all'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 4.42.0 requires urllib3~=2.0, but you have urllib3 1.26.12 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U predibase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matti\\Documents\\GitHub\\SQL_Chatbot_NLP_Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Connected to Predibase as </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">User</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">=</span><span style=\"color: #ffff00; text-decoration-color: #ffff00\">9de258a7</span><span style=\"color: #ffff00; text-decoration-color: #ffff00\">-e0c1-46b3-aa05-6df97a2ef05e</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">username</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">mattia</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">.varagnolo.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">@studenti.unipd.it)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mConnected to Predibase as \u001b[0m\u001b[1;35mUser\u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;34m=\u001b[0m\u001b[93m9de258a7\u001b[0m\u001b[93m-e0c1-46b3-aa05-6df97a2ef05e\u001b[0m\u001b[1;34m, \u001b[0m\n",
       "\u001b[1;33musername\u001b[0m\u001b[1;34m=\u001b[0m\u001b[1;35mmattia\u001b[0m\u001b[1;34m.varagnolo.\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;34m@studenti.unipd.it\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\matti\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaatvo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\matti\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\matti\\Documents\\GitHub\\SQL_Chatbot_NLP_Project\\wandb\\run-20240829_141236-hgcxuecd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maatvo/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/hgcxuecd' target=\"_blank\">charmed-cherry-1</a></strong> to <a href='https://wandb.ai/maatvo/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maatvo/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset' target=\"_blank\">https://wandb.ai/maatvo/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maatvo/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/hgcxuecd' target=\"_blank\">https://wandb.ai/maatvo/Fine-tune%20Llama%203%208B%20on%20SQL%20dataset/runs/hgcxuecd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from predibase import Predibase\n",
    "\n",
    "# Pass api_token directly, or get it from the environment variable.\n",
    "pb = Predibase(api_token=\"pb_P8XqjAqLXTuM-e6j7Se_Sw\")\n",
    "\n",
    "hf_token = \"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\"\n",
    "\n",
    "login(token = hf_token)\n",
    "\n",
    "wb_token = \"094590d0aa8813c0cc044d53c48dbf393da80d96\"\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3 8B on SQL dataset',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "dataset_name = \"gretelai/synthetic_text_to_sql\"\n",
    "new_model = \"llama-3-8b-sql-chatbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "df_dataset = datasets.load_dataset(\n",
    "    \"gretelai/synthetic_text_to_sql\"\n",
    ").get(\"train\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_template = \"<|im_start|>user\\n {prompt} <|im_end|>\\n<|im_start|>assistant\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sql_training_prompt_template = \"\"\"\\\n",
    "    You are a database management system expert, proficient in Structured Query Language (SQL).\n",
    "    \n",
    "    Your job is to write an SQL query that answers the following question, based on the given \\\n",
    "database schema and any additional information provided.  Use SQLite syntax.\n",
    "    \n",
    "    Please output only SQL (without any explanations).\n",
    "\n",
    "\n",
    "    ### Schema: {sql_context}\n",
    "\n",
    "\n",
    "    ### Knowledge: This \"{sql_task_type}\" type task is commonly used for {sql_task_type_description} \\\n",
    "in the domain of {domain}, which involves {domain_description}.\n",
    "\n",
    "\n",
    "    ### Question: {sql_prompt}\n",
    "\n",
    "\n",
    "    ### Completion:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.to_csv(\n",
    "    \"data/fine_tuning/synthetic_text_to_sql_llama-3-8b-instruct.csv\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pb.datasets.from_file(\n",
    "    \"data/fine_tuning/train.csv\",\n",
    "    name=\"gretel_ai_synthetic_text_to_sql_llama-3-8b_train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = pb.repos.create(\n",
    "    name=\"gretel_ai_synthetic_text_to_sql_llama-3-8b-instruct\", \n",
    "    description=\"Fine-tuning on GretelAI text-to-SQL synthetic dataset with Predibase.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Bad request. Response status code 400. Error: {'message': 'Selected dataset gretel_ai_synthetic_text_to_sql_llama-3-8b_train is missing at least one required column: [prompt completion]'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m adapter: FinetuningJob \u001b[38;5;241m=\u001b[39m \u001b[43mpb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetuning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0002\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgretel_ai_synthetic_text_to_sql_llama-3-8b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfine-tune \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-3-8b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m with GretelAI text-to-SQL synthetic dataset (no JSON)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matti\\Documents\\GitHub\\SQL_Chatbot_NLP_Project\\.venv\\Lib\\site-packages\\predibase\\resources\\finetuning_jobs.py:52\u001b[0m, in \u001b[0;36mFinetuningJobs.create\u001b[1;34m(self, config, dataset, repo, description, watch)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repo, Repo):\n\u001b[0;32m     50\u001b[0m     repo \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m---> 52\u001b[0m job_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v2/finetuning/jobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m job \u001b[38;5;241m=\u001b[39m FinetuningJob\u001b[38;5;241m.\u001b[39mmodel_validate(job_resp)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully requested finetuning of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob\u001b[38;5;241m.\u001b[39mtarget_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob\u001b[38;5;241m.\u001b[39mtarget_version_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. (Job UUID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob\u001b[38;5;241m.\u001b[39muuid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\matti\\Documents\\GitHub\\SQL_Chatbot_NLP_Project\\.venv\\Lib\\site-packages\\predibase\\_client.py:160\u001b[0m, in \u001b[0;36mPredibase.http_post\u001b[1;34m(self, endpoint, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;129m@warn_outdated_sdk\u001b[39m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m():\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_gateway \u001b[38;5;241m+\u001b[39m endpoint,\n\u001b[0;32m    154\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    158\u001b[0m     )\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matti\\Documents\\GitHub\\SQL_Chatbot_NLP_Project\\.venv\\Lib\\site-packages\\predibase\\_client.py:207\u001b[0m, in \u001b[0;36m_to_json\u001b[1;34m(resp)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m    206\u001b[0m     payload \u001b[38;5;241m=\u001b[39m payload_json(resp)\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad request. Response status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Error: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpayload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[0;32m    212\u001b[0m     payload \u001b[38;5;241m=\u001b[39m payload_json(resp)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Bad request. Response status code 400. Error: {'message': 'Selected dataset gretel_ai_synthetic_text_to_sql_llama-3-8b_train is missing at least one required column: [prompt completion]'}"
     ]
    }
   ],
   "source": [
    "adapter: FinetuningJob = pb.finetuning.jobs.create(\n",
    "    config={\n",
    "        \"base_model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        \"epochs\": 5,\n",
    "        \"learning_rate\": 0.0002,\n",
    "    },\n",
    "    dataset=dataset,\n",
    "    repo=\"gretel_ai_synthetic_text_to_sql_llama-3-8b-instruct\",\n",
    "    description='fine-tune \"llama-3-8b-instruct\" with GretelAI text-to-SQL synthetic dataset (no JSON)',\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
